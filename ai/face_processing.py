"""
Face Processing Service - Server-side face detection and alignment.

Matches the client-side pipeline exactly:
  1. SCRFD face detection with 5-point landmarks
  2. Umeyama similarity transform alignment
  3. Outputs aligned 112x112 BGR image ready for ArcFace embedding

This ensures that embeddings stored during rebuild are consistent
with embeddings generated by the client during live scanning.
"""

import logging
from pathlib import Path
from typing import List, Optional, Tuple

import cv2
import numpy as np
import onnxruntime as ort

logger = logging.getLogger(__name__)

# ArcFace reference landmarks for 112x112 (same as client FaceDetectionConfig)
ARCFACE_REFERENCE_LANDMARKS_112 = np.array([
    [38.2946, 51.6963],
    [73.5318, 51.5014],
    [56.0252, 71.7366],
    [41.5493, 92.3655],
    [70.7299, 92.2041],
], dtype=np.float64)

SCRFD_INPUT_SIZE = 640
SCRFD_CONFIDENCE_THRESHOLD = 0.5
SCRFD_NMS_THRESHOLD = 0.4
SCRFD_STRIDES = [8, 16, 32]
ALIGNMENT_OUTPUT_SIZE = 112

SCRFD_MODEL_SEARCH_PATHS = (
    "../client/public/scrfd_2.5g.onnx",
    "../model/scrfd_2.5g.onnx",
    "scrfd_2.5g.onnx",
    "./models/scrfd_2.5g.onnx",
)


class FaceProcessingService:
    """
    Server-side face detection + alignment matching the client pipeline.

    Uses SCRFD for landmark detection and Umeyama transform for alignment,
    producing aligned 112x112 BGR images suitable for ArcFace embedding.
    """

    _instance: Optional["FaceProcessingService"] = None

    def __init__(self, scrfd_model_path: Optional[str] = None):
        self._scrfd_path = self._resolve_model_path(scrfd_model_path)
        self._scrfd_session: Optional[ort.InferenceSession] = None
        self._initialize()

    @classmethod
    def get_instance(cls, scrfd_model_path: Optional[str] = None) -> "FaceProcessingService":
        if cls._instance is None:
            cls._instance = cls(scrfd_model_path)
        return cls._instance

    def _resolve_model_path(self, model_path: Optional[str]) -> str:
        if model_path and Path(model_path).exists():
            return model_path
        base = Path(__file__).parent
        for relative in SCRFD_MODEL_SEARCH_PATHS:
            full = base / relative
            if full.exists():
                logger.info(f"Found SCRFD model at: {full}")
                return str(full)
        raise FileNotFoundError(
            "Cannot find scrfd_2.5g.onnx. Searched: " + str(SCRFD_MODEL_SEARCH_PATHS)
        )

    def _initialize(self) -> None:
        logger.info(f"Loading SCRFD model: {self._scrfd_path}")
        self._scrfd_session = ort.InferenceSession(
            self._scrfd_path,
            providers=["CPUExecutionProvider"],
        )
        # Cache output name order — needed to match client-side JS ordering
        self._output_names = [o.name for o in self._scrfd_session.get_outputs()]
        logger.info("SCRFD model loaded for server-side face processing")

    # ─────────────────────────────────────────────
    #  SCRFD detection (matches client parseSCRFDDetections)
    # ─────────────────────────────────────────────

    def _preprocess_for_scrfd(self, image: np.ndarray) -> np.ndarray:
        """
        Preprocess image for SCRFD — matches client preprocessForSCRFD exactly.

        Client uses BGR channel order in the tensor:
            inputArray[i]              = (b - 127.5) / 128
            inputArray[pixelCount + i] = (g - 127.5) / 128
            inputArray[2*pixelCount+i] = (r - 127.5) / 128
        """
        resized = cv2.resize(image, (SCRFD_INPUT_SIZE, SCRFD_INPUT_SIZE))
        # OpenCV is BGR — client sends BGR order to SCRFD too
        img = resized.astype(np.float32)
        img = (img - 127.5) / 128.0  # already BGR from OpenCV
        # HWC → CHW
        img = np.transpose(img, (2, 0, 1))
        return np.expand_dims(img, axis=0).astype(np.float32)

    def _build_output_mapping(self, outputs: list) -> dict:
        """
        Build stride→{scores, boxes, keypoints} mapping that matches the
        client-side JS ordering.

        ONNX Runtime Web returns outputs as a dict keyed by output name.
        JavaScript Object.values() returns integer-like keys in ascending
        numeric order (e.g., 446,449,452,466,469,472,486,489,492).

        Python onnxruntime returns a list ordered by the model graph, which
        may differ (e.g., grouped by type: all scores, all boxes, all kps).

        We sort by numeric name to match the JS ordering, then group into
        stride triplets (scores, boxes, keypoints).
        """
        # Build name→tensor mapping
        name_tensor = {}
        for i, name in enumerate(self._output_names):
            name_tensor[name] = outputs[i]

        # Sort by numeric name (same as JS Object.values on integer keys)
        sorted_names = sorted(name_tensor.keys(), key=lambda x: int(x))

        # Groups of 3: [scores, boxes, keypoints] per stride
        stride_map = {}
        for idx, stride in enumerate(SCRFD_STRIDES):
            base = idx * 3
            stride_map[stride] = {
                "scores": name_tensor[sorted_names[base]],      # shape (N, 1)
                "boxes": name_tensor[sorted_names[base + 1]],    # shape (N, 4)
                "keypoints": name_tensor[sorted_names[base + 2]],  # shape (N, 10)
            }
        return stride_map

    def detect_faces_with_landmarks(
        self, image: np.ndarray
    ) -> List[dict]:
        """
        Detect faces and return list of dicts with 'bbox', 'landmarks', 'conf'.
        Coordinates are in the original image space.

        Matches client-side parseSCRFDDetections logic, vectorized with numpy.
        """
        image_height, image_width = image.shape[:2]
        input_tensor = self._preprocess_for_scrfd(image)

        input_name = self._scrfd_session.get_inputs()[0].name
        outputs = self._scrfd_session.run(None, {input_name: input_tensor})

        stride_outputs = self._build_output_mapping(outputs)

        faces = []
        for stride in SCRFD_STRIDES:
            scores_2d = stride_outputs[stride]["scores"]       # (N, 1)
            boxes_2d = stride_outputs[stride]["boxes"]         # (N, 4)
            keypoints_2d = stride_outputs[stride]["keypoints"] # (N, 10)

            num_anchors = scores_2d.shape[0]
            feature_map_size = SCRFD_INPUT_SIZE // stride
            anchors_per_cell = max(1, num_anchors // (feature_map_size * feature_map_size))

            # Vectorized sigmoid + confidence filter
            confidences = 1.0 / (1.0 + np.exp(-scores_2d[:, 0]))
            mask = confidences >= SCRFD_CONFIDENCE_THRESHOLD
            if not np.any(mask):
                continue

            indices = np.where(mask)[0]
            confs = confidences[indices]

            # Vectorized grid coordinate computation
            grid_indices = indices // anchors_per_cell
            grid_x = grid_indices % feature_map_size
            grid_y = grid_indices // feature_map_size
            anchor_cx = (grid_x + 0.5) * stride
            anchor_cy = (grid_y + 0.5) * stride

            # Vectorized bbox computation
            left = boxes_2d[indices, 0] * stride
            top = boxes_2d[indices, 1] * stride
            right = boxes_2d[indices, 2] * stride
            bottom = boxes_2d[indices, 3] * stride

            scale_x = image_width / SCRFD_INPUT_SIZE
            scale_y = image_height / SCRFD_INPUT_SIZE

            x1 = (anchor_cx - left) * scale_x
            y1 = (anchor_cy - top) * scale_y
            x2 = (anchor_cx + right) * scale_x
            y2 = (anchor_cy + bottom) * scale_y

            # Vectorized landmark computation
            kps = keypoints_2d[indices]  # (K, 10)
            for k in range(len(indices)):
                landmarks = np.zeros((5, 2), dtype=np.float64)
                for j in range(5):
                    landmarks[j, 0] = (anchor_cx[k] + kps[k, j * 2] * stride) * scale_x
                    landmarks[j, 1] = (anchor_cy[k] + kps[k, j * 2 + 1] * stride) * scale_y

                faces.append({
                    "bbox": [float(x1[k]), float(y1[k]), float(x2[k]), float(y2[k])],
                    "landmarks": landmarks,
                    "conf": float(confs[k]),
                })

        # NMS
        faces = self._apply_nms(faces)
        return faces

    def _apply_nms(self, faces: List[dict]) -> List[dict]:
        """Apply Non-Maximum Suppression using OpenCV's optimized C++ implementation."""
        if not faces:
            return []

        boxes = [[f["bbox"][0], f["bbox"][1],
                  f["bbox"][2] - f["bbox"][0],
                  f["bbox"][3] - f["bbox"][1]] for f in faces]
        scores = [f["conf"] for f in faces]

        indices = cv2.dnn.NMSBoxes(
            boxes, scores,
            score_threshold=SCRFD_CONFIDENCE_THRESHOLD,
            nms_threshold=SCRFD_NMS_THRESHOLD,
        )
        if len(indices) == 0:
            return []
        return [faces[i] for i in indices.flatten()]

    def _select_best_face(self, faces: List[dict]) -> Optional[dict]:
        if not faces:
            return None
        return max(
            faces,
            key=lambda f: f["conf"] * max(0, (f["bbox"][2] - f["bbox"][0]) * (f["bbox"][3] - f["bbox"][1])),
        )

    # ─────────────────────────────────────────────
    #  Umeyama alignment (matches client FaceAlignmentService)
    # ─────────────────────────────────────────────

    @staticmethod
    def compute_umeyama_transform(
        source_points: np.ndarray,
        destination_points: np.ndarray,
    ) -> Optional[np.ndarray]:
        """
        Compute Umeyama similarity transform matching the client JS exactly.
        Returns 2x3 affine matrix or None.
        """
        n = len(source_points)
        if n < 2:
            return None

        src = source_points.astype(np.float64)
        dst = destination_points.astype(np.float64)

        src_mean = src.mean(axis=0)
        dst_mean = dst.mean(axis=0)

        src_centered = src - src_mean
        dst_centered = dst - dst_mean

        src_variance = np.sum(src_centered ** 2) / n
        if src_variance < 1e-10:
            return None

        # Covariance matrix (2x2)
        cov = (dst_centered.T @ src_centered) / n

        # SVD of covariance
        U, S, Vt = np.linalg.svd(cov)

        # Handle reflection
        det = np.linalg.det(U) * np.linalg.det(Vt)
        D = np.array([1.0, 1.0 if det >= 0 else -1.0])

        rotation = U @ np.diag(D) @ Vt
        trace_sd = np.sum(S * D)
        scale = trace_sd / src_variance

        translation = dst_mean - scale * rotation @ src_mean

        # Build 2x3 affine matrix
        M = np.zeros((2, 3), dtype=np.float64)
        M[:2, :2] = scale * rotation
        M[:, 2] = translation
        return M

    def align_face(
        self, image: np.ndarray, landmarks: np.ndarray
    ) -> Optional[np.ndarray]:
        """
        Align face using Umeyama transform, outputting 112x112 BGR image.
        Matches the client-side alignFaceToCanvas exactly.
        """
        M = self.compute_umeyama_transform(landmarks, ARCFACE_REFERENCE_LANDMARKS_112)
        if M is None:
            return None

        aligned = cv2.warpAffine(
            image,
            M,
            (ALIGNMENT_OUTPUT_SIZE, ALIGNMENT_OUTPUT_SIZE),
            borderValue=(128, 128, 128),  # gray fill matches client "#808080"
        )
        return aligned

    # ─────────────────────────────────────────────
    #  Full pipeline: image → aligned face
    # ─────────────────────────────────────────────

    def detect_and_align_face(
        self, image: np.ndarray
    ) -> Optional[np.ndarray]:
        """
        Full pipeline: detect face → align → return 112x112 BGR image.
        Returns None if no face found.
        """
        faces = self.detect_faces_with_landmarks(image)
        best = self._select_best_face(faces)
        if best is None:
            logger.warning("No face detected in image")
            return None
        return self.align_face(image, best["landmarks"])

    def detect_and_align_all_faces(
        self, image: np.ndarray
    ) -> List[np.ndarray]:
        """Detect and align all faces in image."""
        faces = self.detect_faces_with_landmarks(image)
        results = []
        for face in faces:
            aligned = self.align_face(image, face["landmarks"])
            if aligned is not None:
                results.append(aligned)
        return results
